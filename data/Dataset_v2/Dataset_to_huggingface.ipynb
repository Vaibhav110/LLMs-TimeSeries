{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e829769-e934-4a6d-acd6-e158dfcf056d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5380ec-8e42-412a-b8b7-91daf1ca816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional\n",
    "dataset = load_dataset(\"nielsr/docvqa_1200_examples\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b784cb-b1bd-4491-87da-b704a76e3416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "        num_rows: 131\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"Output_dataset_image_mContext.json\")\n",
    "\n",
    "# Load images into the dataset\n",
    "def load_image(example):\n",
    "    example[\"image_scalogram\"] = Image.open(example[\"image_path_scalogram\"])\n",
    "    example[\"image_mtf\"] = Image.open(example[\"image_path_mtf\"])\n",
    "    example[\"image_spectogram\"] = Image.open(example[\"image_path_spectrogram\"])\n",
    "    return example\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(load_image)\n",
    "    \n",
    "# Inspect the dataset\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d355845f-fd4a-41b6-a82d-9b7bafde030a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'id_0', 'image_path_scalogram': 'Dataset_LA_10/Images_Scalogram/Data_0.jpg', 'query_scalogram': ' Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday  during the Winter 2016 season is 145, 298, 307, 368, 258, 238, 249, 347, 246, 201. Analyze the provided Scalogram of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday.', 'image_path_mtf': 'Dataset_LA_10/Images_MTF/Data_0.jpg', 'query_mtf': ' Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday  during the Winter 2016 season is 145, 298, 307, 368, 258, 238, 249, 347, 246, 201. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday.', 'image_path_spectrogram': 'Dataset_LA_10/Images_Spectrogram/Data_0.jpg', 'query_spectrogram': ' Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday during the Winter 2016 season is 145, 298, 307, 368, 258, 238, 249, 347, 246, 201.Analyze the provided Spectrogram of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday.', 'answers': '287', 'image_scalogram': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=660x498 at 0x173C8BC8DA0>, 'image_mtf': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x173C8DBF1A0>, 'image_spectogram': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=800x500 at 0x173C8DBF620>}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ae0e3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "    num_rows: 32\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON dataset\n",
    "dataset_test1 = load_dataset(\"json\", data_files=\"Output_dataset_image_test_v1.json\")\n",
    "\n",
    "# Load images into the dataset\n",
    "def load_image(example):\n",
    "    example[\"image_scalogram\"] = Image.open(example[\"image_path_scalogram\"])\n",
    "    example[\"image_mtf\"] = Image.open(example[\"image_path_mtf\"])\n",
    "    example[\"image_spectogram\"] = Image.open(example[\"image_path_spectrogram\"])\n",
    "    return example\n",
    "\n",
    "dataset_test1 = dataset_test1.map(load_image)\n",
    "    \n",
    "# Inspect the dataset\n",
    "print(dataset_test1[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e538f501",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2ff92f282094be4882ace1b98a182c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0aba5e62874446683c0ce8fc6311c3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "    num_rows: 32\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON dataset\n",
    "dataset_test2 = load_dataset(\"json\", data_files=\"Output_dataset_image_test_v2.json\")\n",
    "\n",
    "# Load images into the dataset\n",
    "def load_image(example):\n",
    "    example[\"image_scalogram\"] = Image.open(example[\"image_path_scalogram\"])\n",
    "    example[\"image_mtf\"] = Image.open(example[\"image_path_mtf\"])\n",
    "    example[\"image_spectogram\"] = Image.open(example[\"image_path_spectrogram\"])\n",
    "    return example\n",
    "\n",
    "dataset_test2 = dataset_test2.map(load_image)\n",
    "    \n",
    "# Inspect the dataset\n",
    "print(dataset_test2[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153742c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432e1d568de14b37b2e7f4e99d71a6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148ebad8a04745d8ab1fefc2f63fe1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON dataset\n",
    "dataset_test3 = load_dataset(\"json\", data_files=\"Output_dataset_image_test_v3.json\")\n",
    "\n",
    "# Load images into the dataset\n",
    "def load_image(example):\n",
    "    example[\"image_scalogram\"] = Image.open(example[\"image_path_scalogram\"])\n",
    "    example[\"image_mtf\"] = Image.open(example[\"image_path_mtf\"])\n",
    "    example[\"image_spectogram\"] = Image.open(example[\"image_path_spectrogram\"])\n",
    "    return example\n",
    "\n",
    "dataset_test3 = dataset_test3.map(load_image)\n",
    "    \n",
    "# Inspect the dataset\n",
    "print(dataset_test3[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5dfda5c1-e54c-4b4e-b8f1-8f5950183181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "        num_rows: 131\n",
      "    })\n",
      "    test1: Dataset({\n",
      "        features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "        num_rows: 32\n",
      "    })\n",
      "    test2: Dataset({\n",
      "        features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "        num_rows: 32\n",
      "    })\n",
      "    test3: Dataset({\n",
      "        features: ['id', 'image_path_scalogram', 'query_scalogram', 'image_path_mtf', 'query_mtf', 'image_path_spectrogram', 'query_spectrogram', 'answers', 'image_scalogram', 'image_mtf', 'image_spectogram'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Combine into a DatasetDict\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset[\"train\"],\n",
    "    \"test1\": dataset_test1[\"train\"],\n",
    "    \"test2\": dataset_test2[\"train\"],\n",
    "    \"test3\": dataset_test3[\"train\"]\n",
    "})\n",
    "\n",
    "print(dataset_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec193b0-6ba0-4195-a95c-0589cac7992d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bcbff02-e1dc-4a80-a58d-a62b369e3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face Hub\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1825b7a-a7bc-4b17-a727-c6377903d375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5043e430e4e8423ba9f40b4b9462545c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427b9b28b53c4d94b4a41998d1b89447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/131 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c4ec90fd8a46f988c764a6b4c84e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60876119a0a4cb1bb5b1de28dec6e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6944ab36209a4fb8a498da24aac0bf09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf778eb6dfc4986aa3cd3b31008eba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7838783b8b442a8aa45920bc16d2d81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb898355bb742daaeff8f39f96c5220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f832e8685b441f28db3ee82f21f825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7398476123347309e800b45060b969c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba872cbbfa14742910de1f7eb5626ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33368419a399490ba0b7ab8901708f7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/VaibhavMal/AirQualty_imageConv_2/commit/ce3cd62297ea97ac5d1a93550551761d9d626c47', commit_message='Upload dataset', commit_description='', oid='ce3cd62297ea97ac5d1a93550551761d9d626c47', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/VaibhavMal/AirQualty_imageConv_2', endpoint='https://huggingface.co', repo_type='dataset', repo_id='VaibhavMal/AirQualty_imageConv_2'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push dataset to the Hub\n",
    "dataset_dict.push_to_hub(\"VaibhavMal/AirQualty_imageConv_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12753d3-c95b-482d-9e80-142058b4f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class LlavaDataset():\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LLaVa. This class takes a HuggingFace Dataset as input.\n",
    "\n",
    "    Each row, consists of image path(png/jpg/jpeg) and ground truth data (json/jsonl/txt).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.answer_token_sequences = []\n",
    "        self.query_list = []\n",
    "        for sample in self.dataset:\n",
    "            if \"answers\" in sample:\n",
    "                assert isinstance(sample[\"answers\"], list)\n",
    "                self.answer_token_sequences.append(sample[\"answers\"])\n",
    "                self.query_list.append(sample[\"query\"])\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns one item of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            image : the original Receipt image\n",
    "            target_sequence : tokenized ground truth sequence\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        image = sample[\"image\"]\n",
    "        en_query = self.query_list[idx]\n",
    "        target_sequence = random.choice(self.answer_token_sequences[idx]) # can be more than one, e.g., DocVQA Task 1\n",
    "        return image, en_query, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fac9f1-b9fd-40e4-9604-f835eddda4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"VaibhavMal/AirQualty_imageConv\")\n",
    "print(dataset)\n",
    "\n",
    "sample_data = dataset[\"train\"][0]\n",
    "\n",
    "print(sample_data[\"answers\"])\n",
    "print(isinstance(sample_data[\"answers\"], list))\n",
    "\n",
    "for sample in dataset:\n",
    "    if \"answers\" in sample:\n",
    "        #assert isinstance(sample[\"answers\"], list)\n",
    "        print(sample)\n",
    "        break;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7b05cc-0cfa-418a-bd4d-9e1d4a0931f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LlavaDataset(\"nielsr/docvqa_1200_examples\",  split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1fbab0-4bac-451f-bc79-5ddbcc8c3ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663ff27a-6a7a-412e-9e36-50dc5cc45016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
