                                                                                                                                                            
{'loss': 1.6934, 'grad_norm': 4.359889030456543, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 1.5152, 'grad_norm': 3.7138609886169434, 'learning_rate': 8e-05, 'epoch': 0.11}
{'loss': 1.8734, 'grad_norm': 6.0639824867248535, 'learning_rate': 0.00012, 'epoch': 0.16}
{'loss': 1.6654, 'grad_norm': 3.2244837284088135, 'learning_rate': 0.00016, 'epoch': 0.22}
{'loss': 1.3944, 'grad_norm': 1.7484736442565918, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.2885, 'grad_norm': 1.7483915090560913, 'learning_rate': 0.00019354838709677422, 'epoch': 0.32}
{'loss': 1.3985, 'grad_norm': 2.0781562328338623, 'learning_rate': 0.0001870967741935484, 'epoch': 0.38}
{'loss': 1.6172, 'grad_norm': 2.4941537380218506, 'learning_rate': 0.00018064516129032257, 'epoch': 0.43}
{'loss': 1.4081, 'grad_norm': 1.980513334274292, 'learning_rate': 0.00017419354838709678, 'epoch': 0.49}
{'loss': 1.512, 'grad_norm': 1.7218306064605713, 'learning_rate': 0.00016774193548387098, 'epoch': 0.54}
{'loss': 1.5149, 'grad_norm': 1.5668467283248901, 'learning_rate': 0.00016129032258064516, 'epoch': 0.59}
{'loss': 1.5266, 'grad_norm': 2.0778141021728516, 'learning_rate': 0.00015483870967741937, 'epoch': 0.65}
{'loss': 1.3083, 'grad_norm': 1.2851827144622803, 'learning_rate': 0.00014838709677419355, 'epoch': 0.7}
{'loss': 1.1804, 'grad_norm': 1.3740371465682983, 'learning_rate': 0.00014193548387096775, 'epoch': 0.76}
{'loss': 1.4616, 'grad_norm': 1.5892870426177979, 'learning_rate': 0.00013548387096774193, 'epoch': 0.81}
{'loss': 1.2865, 'grad_norm': 1.5339012145996094, 'learning_rate': 0.00012903225806451613, 'epoch': 0.86}
{'loss': 1.172, 'grad_norm': 0.9212366342544556, 'learning_rate': 0.00012258064516129034, 'epoch': 0.92}
{'loss': 1.2273, 'grad_norm': 0.7847761511802673, 'learning_rate': 0.00011612903225806453, 'epoch': 0.97}
Traceback (most recent call last):                                                                                                                          
{'eval_loss': 1.053897500038147, 'eval_runtime': 0.6478, 'eval_samples_per_second': 12.349, 'eval_steps_per_second': 6.175, 'epoch': 0.97}
{'loss': 1.8917, 'grad_norm': 1.4437943696975708, 'learning_rate': 0.00010967741935483871, 'epoch': 1.03}
{'loss': 1.4949, 'grad_norm': 1.2317335605621338, 'learning_rate': 0.0001032258064516129, 'epoch': 1.08}
{'loss': 1.094, 'grad_norm': 0.9529461860656738, 'learning_rate': 9.677419354838711e-05, 'epoch': 1.14}
{'loss': 1.2402, 'grad_norm': 0.7236601710319519, 'learning_rate': 9.032258064516129e-05, 'epoch': 1.19}
{'loss': 1.4101, 'grad_norm': 1.0511863231658936, 'learning_rate': 8.387096774193549e-05, 'epoch': 1.24}
{'loss': 1.0695, 'grad_norm': 1.2821992635726929, 'learning_rate': 7.741935483870968e-05, 'epoch': 1.3}
{'loss': 1.1622, 'grad_norm': 0.8884444236755371, 'learning_rate': 7.096774193548388e-05, 'epoch': 1.35}
{'loss': 1.0673, 'grad_norm': 1.4984092712402344, 'learning_rate': 6.451612903225807e-05, 'epoch': 1.41}
{'loss': 1.3026, 'grad_norm': 0.8705312609672546, 'learning_rate': 5.8064516129032266e-05, 'epoch': 1.46}
{'loss': 1.3004, 'grad_norm': 1.3209666013717651, 'learning_rate': 5.161290322580645e-05, 'epoch': 1.51}
{'loss': 1.217, 'grad_norm': 0.8250613212585449, 'learning_rate': 4.516129032258064e-05, 'epoch': 1.57}
{'loss': 1.37, 'grad_norm': 1.7126071453094482, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.62}
{'loss': 1.0791, 'grad_norm': 0.7446141242980957, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.68}
{'loss': 1.1104, 'grad_norm': 0.9489895701408386, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.73}
{'loss': 1.3513, 'grad_norm': 1.1003539562225342, 'learning_rate': 1.935483870967742e-05, 'epoch': 1.78}
{'loss': 1.1061, 'grad_norm': 1.312154769897461, 'learning_rate': 1.2903225806451613e-05, 'epoch': 1.84}
{'loss': 1.1921, 'grad_norm': 1.0217781066894531, 'learning_rate': 6.451612903225806e-06, 'epoch': 1.89}
{'loss': 1.2747, 'grad_norm': 1.7104452848434448, 'learning_rate': 0.0, 'epoch': 1.95}
{'eval_loss': 1.0507304668426514, 'eval_runtime': 0.1337, 'eval_samples_per_second': 59.84, 'eval_steps_per_second': 29.92, 'epoch': 1.95}
{'train_runtime': 22.6741, 'train_samples_per_second': 13.055, 'train_steps_per_second': 1.588, 'train_loss': 1.354913314183553, 'epoch': 1.95}
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 35, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 1534, in dot
    return semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype, _builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1355, in dot
    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.options)
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1352, in assert_dtypes_valid
    assert lhs_dtype == rhs_dtype, f"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!"
           ^^^^^^^^^^^^^^^^^^^^^^
AssertionError: First input (bf16) and second input (fp16) must have the same dtype!

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 124, in <module>
    main(args)
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 87, in main
    eval_stats = trainer.evaluate()
                 ^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3975, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4385, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/_utils.py", line 1028, in _unsloth_pre_compute_loss
    return self._old_compute_loss(model, inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 1084, in PeftModelForCausalLM_fast_forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 986, in _CausalLM_fast_forward
    loss = fused_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py", line 147, in fused_linear_cross_entropy
    loss = linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/linear_cross_entropy.py", line 58, in linear_cross_entropy
    return cce_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 168, in cce_linear_cross_entropy
    return linear_cross_entropy_apply(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 125, in linear_cross_entropy_apply
    loss = LinearCrossEntropyFunction.apply(e, c, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 46, in forward
    ret = cce_lse_forward_kernel(
          ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce_lse_forward.py", line 182, in cce_lse_forward_kernel
    _cce_lse_forward_kernel[grid](
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 345, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 662, in run
    kernel = self.compile(
             ^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 276, in compile
    module = src.make_ir(options, codegen_fns, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 113, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 60:16:

    accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)
    for d in range(0, tl.cdiv(D, BLOCK_D)):
        # Load the next block of A and B, generate a mask by checking the K dimension.
        # If it is out of bounds, set it to 0.
        if EVEN_D:
            e = tl.load(e_ptrs)
            c = tl.load(c_ptrs)
        else:
            e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)
            c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)
        accum = tl.dot(e, c, accum, input_precision=DOT_PRECISION)
                ^
