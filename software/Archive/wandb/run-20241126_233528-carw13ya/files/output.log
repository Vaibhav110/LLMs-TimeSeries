                                                                                                                                                            
{'loss': 1.6934, 'grad_norm': 4.375, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 1.5152, 'grad_norm': 3.71875, 'learning_rate': 8e-05, 'epoch': 0.11}
{'loss': 1.8767, 'grad_norm': 6.15625, 'learning_rate': 0.00012, 'epoch': 0.16}
{'loss': 1.6656, 'grad_norm': 3.25, 'learning_rate': 0.00016, 'epoch': 0.22}
{'loss': 1.3922, 'grad_norm': 1.71875, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.2877, 'grad_norm': 1.75, 'learning_rate': 0.00019354838709677422, 'epoch': 0.32}
{'loss': 1.3998, 'grad_norm': 2.03125, 'learning_rate': 0.0001870967741935484, 'epoch': 0.38}
{'loss': 1.6124, 'grad_norm': 2.484375, 'learning_rate': 0.00018064516129032257, 'epoch': 0.43}
{'loss': 1.4095, 'grad_norm': 2.0, 'learning_rate': 0.00017419354838709678, 'epoch': 0.49}
{'loss': 1.5139, 'grad_norm': 1.7265625, 'learning_rate': 0.00016774193548387098, 'epoch': 0.54}
{'loss': 1.5207, 'grad_norm': 1.578125, 'learning_rate': 0.00016129032258064516, 'epoch': 0.59}
{'loss': 1.5262, 'grad_norm': 2.078125, 'learning_rate': 0.00015483870967741937, 'epoch': 0.65}
{'loss': 1.3088, 'grad_norm': 1.2734375, 'learning_rate': 0.00014838709677419355, 'epoch': 0.7}
{'loss': 1.1797, 'grad_norm': 1.3671875, 'learning_rate': 0.00014193548387096775, 'epoch': 0.76}
{'loss': 1.4622, 'grad_norm': 1.5859375, 'learning_rate': 0.00013548387096774193, 'epoch': 0.81}
{'loss': 1.2852, 'grad_norm': 1.53125, 'learning_rate': 0.00012903225806451613, 'epoch': 0.86}
{'loss': 1.1726, 'grad_norm': 0.91796875, 'learning_rate': 0.00012258064516129034, 'epoch': 0.92}
{'loss': 1.2278, 'grad_norm': 0.7890625, 'learning_rate': 0.00011612903225806453, 'epoch': 0.97}
Traceback (most recent call last):                                                                                                                          
{'eval_loss': 1.0555764436721802, 'eval_runtime': 0.1393, 'eval_samples_per_second': 57.44, 'eval_steps_per_second': 28.72, 'epoch': 0.97}
{'loss': 1.8926, 'grad_norm': 1.421875, 'learning_rate': 0.00010967741935483871, 'epoch': 1.03}
{'loss': 1.4917, 'grad_norm': 1.21875, 'learning_rate': 0.0001032258064516129, 'epoch': 1.08}
{'loss': 1.0967, 'grad_norm': 0.9453125, 'learning_rate': 9.677419354838711e-05, 'epoch': 1.14}
{'loss': 1.2426, 'grad_norm': 0.71875, 'learning_rate': 9.032258064516129e-05, 'epoch': 1.19}
{'loss': 1.4125, 'grad_norm': 1.0390625, 'learning_rate': 8.387096774193549e-05, 'epoch': 1.24}
{'loss': 1.0696, 'grad_norm': 1.25, 'learning_rate': 7.741935483870968e-05, 'epoch': 1.3}
{'loss': 1.1647, 'grad_norm': 0.859375, 'learning_rate': 7.096774193548388e-05, 'epoch': 1.35}
{'loss': 1.0682, 'grad_norm': 1.46875, 'learning_rate': 6.451612903225807e-05, 'epoch': 1.41}
{'loss': 1.2996, 'grad_norm': 0.83984375, 'learning_rate': 5.8064516129032266e-05, 'epoch': 1.46}
{'loss': 1.301, 'grad_norm': 1.2890625, 'learning_rate': 5.161290322580645e-05, 'epoch': 1.51}
{'loss': 1.2183, 'grad_norm': 0.8125, 'learning_rate': 4.516129032258064e-05, 'epoch': 1.57}
{'loss': 1.3647, 'grad_norm': 1.6640625, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.62}
{'loss': 1.0786, 'grad_norm': 0.7421875, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.68}
{'loss': 1.1104, 'grad_norm': 0.94921875, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.73}
{'loss': 1.3516, 'grad_norm': 1.1015625, 'learning_rate': 1.935483870967742e-05, 'epoch': 1.78}
{'loss': 1.1101, 'grad_norm': 1.328125, 'learning_rate': 1.2903225806451613e-05, 'epoch': 1.84}
{'loss': 1.1934, 'grad_norm': 1.015625, 'learning_rate': 6.451612903225806e-06, 'epoch': 1.89}
{'loss': 1.2817, 'grad_norm': 1.71875, 'learning_rate': 0.0, 'epoch': 1.95}
{'eval_loss': 1.0555274486541748, 'eval_runtime': 0.0965, 'eval_samples_per_second': 82.925, 'eval_steps_per_second': 41.462, 'epoch': 1.95}
{'train_runtime': 16.4991, 'train_samples_per_second': 17.94, 'train_steps_per_second': 2.182, 'train_loss': 1.355486164490382, 'epoch': 1.95}
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 35, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 1534, in dot
    return semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype, _builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1355, in dot
    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.options)
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1352, in assert_dtypes_valid
    assert lhs_dtype == rhs_dtype, f"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!"
           ^^^^^^^^^^^^^^^^^^^^^^
AssertionError: First input (bf16) and second input (fp16) must have the same dtype!

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 109, in <module>
    main(args)
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 72, in main
    eval_stats = trainer.evaluate()
                 ^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3975, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4385, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/_utils.py", line 1028, in _unsloth_pre_compute_loss
    return self._old_compute_loss(model, inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 1084, in PeftModelForCausalLM_fast_forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 986, in _CausalLM_fast_forward
    loss = fused_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py", line 147, in fused_linear_cross_entropy
    loss = linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/linear_cross_entropy.py", line 58, in linear_cross_entropy
    return cce_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 168, in cce_linear_cross_entropy
    return linear_cross_entropy_apply(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 125, in linear_cross_entropy_apply
    loss = LinearCrossEntropyFunction.apply(e, c, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 46, in forward
    ret = cce_lse_forward_kernel(
          ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce_lse_forward.py", line 182, in cce_lse_forward_kernel
    _cce_lse_forward_kernel[grid](
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 345, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 662, in run
    kernel = self.compile(
             ^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 276, in compile
    module = src.make_ir(options, codegen_fns, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 113, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 60:16:

    accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)
    for d in range(0, tl.cdiv(D, BLOCK_D)):
        # Load the next block of A and B, generate a mask by checking the K dimension.
        # If it is out of bounds, set it to 0.
        if EVEN_D:
            e = tl.load(e_ptrs)
            c = tl.load(c_ptrs)
        else:
            e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)
            c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)
        accum = tl.dot(e, c, accum, input_precision=DOT_PRECISION)
                ^
