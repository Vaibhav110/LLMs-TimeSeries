                                                                                                                                                                      
{'loss': 1.6934, 'grad_norm': 4.353124141693115, 'learning_rate': 4e-05, 'epoch': 0.05}
{'loss': 1.5152, 'grad_norm': 3.7147319316864014, 'learning_rate': 8e-05, 'epoch': 0.11}
{'loss': 1.8709, 'grad_norm': 6.090845108032227, 'learning_rate': 0.00012, 'epoch': 0.16}
{'loss': 1.6661, 'grad_norm': 3.298797607421875, 'learning_rate': 0.00016, 'epoch': 0.22}
{'loss': 1.3944, 'grad_norm': 1.7572587728500366, 'learning_rate': 0.0002, 'epoch': 0.27}
{'loss': 1.2899, 'grad_norm': 1.758207082748413, 'learning_rate': 0.00019354838709677422, 'epoch': 0.32}
{'loss': 1.4037, 'grad_norm': 2.0637145042419434, 'learning_rate': 0.0001870967741935484, 'epoch': 0.38}
{'loss': 1.6192, 'grad_norm': 2.4818084239959717, 'learning_rate': 0.00018064516129032257, 'epoch': 0.43}
{'loss': 1.4116, 'grad_norm': 1.9864698648452759, 'learning_rate': 0.00017419354838709678, 'epoch': 0.49}
{'loss': 1.5122, 'grad_norm': 1.708184003829956, 'learning_rate': 0.00016774193548387098, 'epoch': 0.54}
{'loss': 1.5147, 'grad_norm': 1.5526148080825806, 'learning_rate': 0.00016129032258064516, 'epoch': 0.59}
{'loss': 1.5181, 'grad_norm': 2.050868272781372, 'learning_rate': 0.00015483870967741937, 'epoch': 0.65}
{'loss': 1.309, 'grad_norm': 1.2793875932693481, 'learning_rate': 0.00014838709677419355, 'epoch': 0.7}
{'loss': 1.1758, 'grad_norm': 1.3683308362960815, 'learning_rate': 0.00014193548387096775, 'epoch': 0.76}
{'loss': 1.4613, 'grad_norm': 1.5858101844787598, 'learning_rate': 0.00013548387096774193, 'epoch': 0.81}
{'loss': 1.2862, 'grad_norm': 1.5309454202651978, 'learning_rate': 0.00012903225806451613, 'epoch': 0.86}
{'loss': 1.1712, 'grad_norm': 0.9171227812767029, 'learning_rate': 0.00012258064516129034, 'epoch': 0.92}
{'loss': 1.2257, 'grad_norm': 0.786765456199646, 'learning_rate': 0.00011612903225806453, 'epoch': 0.97}
Traceback (most recent call last):                                                                                                                                    
{'eval_loss': 1.054034948348999, 'eval_runtime': 0.7197, 'eval_samples_per_second': 11.116, 'eval_steps_per_second': 5.558, 'epoch': 0.97}
{'loss': 1.8935, 'grad_norm': 1.4523526430130005, 'learning_rate': 0.00010967741935483871, 'epoch': 1.03}
{'loss': 1.4934, 'grad_norm': 1.226715326309204, 'learning_rate': 0.0001032258064516129, 'epoch': 1.08}
{'loss': 1.0939, 'grad_norm': 0.9464330673217773, 'learning_rate': 9.677419354838711e-05, 'epoch': 1.14}
{'loss': 1.2411, 'grad_norm': 0.7184211015701294, 'learning_rate': 9.032258064516129e-05, 'epoch': 1.19}
{'loss': 1.4105, 'grad_norm': 1.0504810810089111, 'learning_rate': 8.387096774193549e-05, 'epoch': 1.24}
{'loss': 1.0683, 'grad_norm': 1.2747321128845215, 'learning_rate': 7.741935483870968e-05, 'epoch': 1.3}
{'loss': 1.1646, 'grad_norm': 0.8851976990699768, 'learning_rate': 7.096774193548388e-05, 'epoch': 1.35}
{'loss': 1.068, 'grad_norm': 1.4926328659057617, 'learning_rate': 6.451612903225807e-05, 'epoch': 1.41}
{'loss': 1.3025, 'grad_norm': 0.8640680909156799, 'learning_rate': 5.8064516129032266e-05, 'epoch': 1.46}
{'loss': 1.2994, 'grad_norm': 1.3137389421463013, 'learning_rate': 5.161290322580645e-05, 'epoch': 1.51}
{'loss': 1.2182, 'grad_norm': 0.8233169317245483, 'learning_rate': 4.516129032258064e-05, 'epoch': 1.57}
{'loss': 1.3658, 'grad_norm': 1.6902004480361938, 'learning_rate': 3.870967741935484e-05, 'epoch': 1.62}
{'loss': 1.0805, 'grad_norm': 0.7424300909042358, 'learning_rate': 3.2258064516129034e-05, 'epoch': 1.68}
{'loss': 1.11, 'grad_norm': 0.9513407349586487, 'learning_rate': 2.5806451612903226e-05, 'epoch': 1.73}
{'loss': 1.349, 'grad_norm': 1.088199496269226, 'learning_rate': 1.935483870967742e-05, 'epoch': 1.78}
{'loss': 1.1044, 'grad_norm': 1.309360146522522, 'learning_rate': 1.2903225806451613e-05, 'epoch': 1.84}
{'loss': 1.1944, 'grad_norm': 1.0155805349349976, 'learning_rate': 6.451612903225806e-06, 'epoch': 1.89}
{'loss': 1.2716, 'grad_norm': 1.7130473852157593, 'learning_rate': 0.0, 'epoch': 1.95}
{'eval_loss': 1.0519407987594604, 'eval_runtime': 0.1767, 'eval_samples_per_second': 45.265, 'eval_steps_per_second': 22.633, 'epoch': 1.95}
{'train_runtime': 313.7747, 'train_samples_per_second': 0.943, 'train_steps_per_second': 0.115, 'train_loss': 1.3546608553992376, 'epoch': 1.95}
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 35, in wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/core.py", line 1534, in dot
    return semantic.dot(input, other, acc, input_precision, max_num_imprecise_acc, out_dtype, _builder)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1355, in dot
    assert_dtypes_valid(lhs.dtype, rhs.dtype, builder.options)
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/language/semantic.py", line 1352, in assert_dtypes_valid
    assert lhs_dtype == rhs_dtype, f"First input ({lhs_dtype}) and second input ({rhs_dtype}) must have the same dtype!"
           ^^^^^^^^^^^^^^^^^^^^^^
AssertionError: First input (bf16) and second input (fp16) must have the same dtype!

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 107, in <module>
    main(args)
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 70, in main
    eval_stats = trainer.evaluate()
                 ^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3975, in evaluate
    output = eval_loop(
             ^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4169, in evaluation_loop
    losses, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 4385, in prediction_step
    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/_utils.py", line 1028, in _unsloth_pre_compute_loss
    return self._old_compute_loss(model, inputs, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/transformers/trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 823, in forward
    return model_forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/utils/operations.py", line 811, in __call__
    return convert_to_fp32(self.model_forward(*args, **kwargs))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/amp/autocast_mode.py", line 44, in decorate_autocast
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 1084, in PeftModelForCausalLM_fast_forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/peft/tuners/tuners_utils.py", line 197, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth/models/llama.py", line 986, in _CausalLM_fast_forward
    loss = fused_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/unsloth_zoo/loss_utils.py", line 147, in fused_linear_cross_entropy
    loss = linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/linear_cross_entropy.py", line 58, in linear_cross_entropy
    return cce_linear_cross_entropy(
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 168, in cce_linear_cross_entropy
    return linear_cross_entropy_apply(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 125, in linear_cross_entropy_apply
    loss = LinearCrossEntropyFunction.apply(e, c, params)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce.py", line 46, in forward
    ret = cce_lse_forward_kernel(
          ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/cut_cross_entropy/cce_lse_forward.py", line 182, in cce_lse_forward_kernel
    _cce_lse_forward_kernel[grid](
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 345, in <lambda>
    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/autotuner.py", line 338, in run
    return self.fn.run(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/runtime/jit.py", line 662, in run
    kernel = self.compile(
             ^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 276, in compile
    module = src.make_ir(options, codegen_fns, context)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/triton/compiler/compiler.py", line 113, in make_ir
    return ast_to_ttir(self.fn, self, context=context, options=options, codegen_fns=codegen_fns)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
triton.compiler.errors.CompilationError: at 60:16:

    accum = tl.zeros((BLOCK_B, BLOCK_V), dtype=tl.float32)
    for d in range(0, tl.cdiv(D, BLOCK_D)):
        # Load the next block of A and B, generate a mask by checking the K dimension.
        # If it is out of bounds, set it to 0.
        if EVEN_D:
            e = tl.load(e_ptrs)
            c = tl.load(c_ptrs)
        else:
            e = tl.load(e_ptrs, mask=offs_d[None, :] < D - d * BLOCK_D, other=0.0)
            c = tl.load(c_ptrs, mask=offs_d[:, None] < D - d * BLOCK_D, other=0.0)
        accum = tl.dot(e, c, accum, input_precision=DOT_PRECISION)
                ^
