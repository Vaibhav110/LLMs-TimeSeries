                                                                                                                                                            
{'loss': 0.8678, 'grad_norm': 0.318359375, 'learning_rate': 4e-05, 'epoch': 0.0}
{'loss': 0.9631, 'grad_norm': 0.37890625, 'learning_rate': 8e-05, 'epoch': 0.0}
{'loss': 1.1673, 'grad_norm': 0.40625, 'learning_rate': 0.00012, 'epoch': 0.0}
{'loss': 1.0509, 'grad_norm': 0.349609375, 'learning_rate': 0.00016, 'epoch': 0.0}
{'loss': 0.841, 'grad_norm': 0.326171875, 'learning_rate': 0.0002, 'epoch': 0.0}
{'loss': 1.0631, 'grad_norm': 0.353515625, 'learning_rate': 0.00019999199839967994, 'epoch': 0.0}
{'loss': 0.7537, 'grad_norm': 0.28515625, 'learning_rate': 0.0001999839967993599, 'epoch': 0.0}
{'loss': 1.1159, 'grad_norm': 0.333984375, 'learning_rate': 0.00019997599519903982, 'epoch': 0.0}
{'loss': 1.0081, 'grad_norm': 0.265625, 'learning_rate': 0.00019996799359871977, 'epoch': 0.0}
{'loss': 0.8818, 'grad_norm': 0.267578125, 'learning_rate': 0.0001999599919983997, 'epoch': 0.0}
{'loss': 1.0005, 'grad_norm': 0.2265625, 'learning_rate': 0.00019995199039807963, 'epoch': 0.0}
{'loss': 1.2545, 'grad_norm': 0.275390625, 'learning_rate': 0.00019994398879775955, 'epoch': 0.0}
{'loss': 1.0642, 'grad_norm': 0.322265625, 'learning_rate': 0.00019993598719743948, 'epoch': 0.0}
{'loss': 0.7489, 'grad_norm': 0.26171875, 'learning_rate': 0.00019992798559711943, 'epoch': 0.0}
{'loss': 1.0034, 'grad_norm': 0.2431640625, 'learning_rate': 0.00019991998399679936, 'epoch': 0.0}
{'loss': 0.739, 'grad_norm': 0.287109375, 'learning_rate': 0.00019991198239647931, 'epoch': 0.0}
{'loss': 1.1463, 'grad_norm': 0.30078125, 'learning_rate': 0.00019990398079615924, 'epoch': 0.0}
{'loss': 0.9775, 'grad_norm': 0.359375, 'learning_rate': 0.00019989597919583917, 'epoch': 0.0}
{'loss': 0.878, 'grad_norm': 0.2451171875, 'learning_rate': 0.00019988797759551912, 'epoch': 0.0}
{'loss': 1.0122, 'grad_norm': 0.32421875, 'learning_rate': 0.00019987997599519905, 'epoch': 0.0}
{'loss': 0.9929, 'grad_norm': 0.31640625, 'learning_rate': 0.000199871974394879, 'epoch': 0.0}
{'loss': 0.9829, 'grad_norm': 0.2734375, 'learning_rate': 0.00019986397279455893, 'epoch': 0.0}
{'loss': 1.139, 'grad_norm': 0.240234375, 'learning_rate': 0.00019985597119423886, 'epoch': 0.0}
{'loss': 0.9872, 'grad_norm': 0.333984375, 'learning_rate': 0.00019984796959391878, 'epoch': 0.0}
{'loss': 0.7596, 'grad_norm': 0.2578125, 'learning_rate': 0.0001998399679935987, 'epoch': 0.0}
{'loss': 0.9474, 'grad_norm': 0.283203125, 'learning_rate': 0.00019983196639327867, 'epoch': 0.0}
{'loss': 0.9697, 'grad_norm': 0.34765625, 'learning_rate': 0.0001998239647929586, 'epoch': 0.0}
{'loss': 0.8989, 'grad_norm': 0.3125, 'learning_rate': 0.00019981596319263855, 'epoch': 0.0}
{'loss': 1.2516, 'grad_norm': 0.287109375, 'learning_rate': 0.00019980796159231847, 'epoch': 0.0}
{'loss': 1.1775, 'grad_norm': 0.31640625, 'learning_rate': 0.0001997999599919984, 'epoch': 0.0}
{'loss': 0.8138, 'grad_norm': 0.302734375, 'learning_rate': 0.00019979195839167835, 'epoch': 0.0}
{'loss': 0.6321, 'grad_norm': 0.21875, 'learning_rate': 0.00019978395679135828, 'epoch': 0.0}
{'loss': 0.7491, 'grad_norm': 0.322265625, 'learning_rate': 0.00019977595519103824, 'epoch': 0.0}
{'loss': 0.6931, 'grad_norm': 0.25390625, 'learning_rate': 0.00019976795359071816, 'epoch': 0.0}
{'loss': 0.8782, 'grad_norm': 0.283203125, 'learning_rate': 0.0001997599519903981, 'epoch': 0.0}
{'loss': 1.102, 'grad_norm': 0.310546875, 'learning_rate': 0.00019975195039007804, 'epoch': 0.0}
{'loss': 1.0233, 'grad_norm': 0.328125, 'learning_rate': 0.00019974394878975794, 'epoch': 0.0}
{'loss': 0.8153, 'grad_norm': 0.248046875, 'learning_rate': 0.0001997359471894379, 'epoch': 0.0}
{'loss': 0.8778, 'grad_norm': 0.28515625, 'learning_rate': 0.00019972794558911782, 'epoch': 0.0}
{'loss': 1.125, 'grad_norm': 0.2734375, 'learning_rate': 0.00019971994398879775, 'epoch': 0.0}
{'loss': 0.8497, 'grad_norm': 0.31640625, 'learning_rate': 0.0001997119423884777, 'epoch': 0.0}
{'loss': 1.1757, 'grad_norm': 0.310546875, 'learning_rate': 0.00019970394078815763, 'epoch': 0.0}
{'loss': 0.8711, 'grad_norm': 0.259765625, 'learning_rate': 0.00019969593918783759, 'epoch': 0.0}
{'loss': 0.9507, 'grad_norm': 0.294921875, 'learning_rate': 0.0001996879375875175, 'epoch': 0.0}
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 113, in <module>
    main(args)
  File "/home/vaibhav/LLMs-TimeSeries/software/Archive/LLM_finetune.py", line 75, in main
    trainer_stats = trainer.train()
                    ^^^^^^^^^^^^^^^
  File "<string>", line 157, in train
  File "<string>", line 380, in _fast_inner_training_loop
  File "<string>", line 64, in _unsloth_training_step
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/accelerate/accelerator.py", line 2241, in backward
    loss.backward(**kwargs)
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/_tensor.py", line 581, in backward
    torch.autograd.backward(
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/autograd/__init__.py", line 347, in backward
    _engine_run_backward(
  File "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/torch/autograd/graph.py", line 825, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
