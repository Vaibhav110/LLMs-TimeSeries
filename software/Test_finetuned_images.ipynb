{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, BitsAndBytesConfig, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "\n",
    "# Define quantization config\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# Load the base model with adapters on top\n",
    "model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "    \"/home/vaibhav/LLMs-TimeSeries/software/Model_Dataset_v2_LA_10_mtf\",\n",
    "    # MODEL_ID,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=quantization_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"VaibhavMal/AirQualty_imageConv_2\", split=\"test3\")\n",
    "# dataset_train = load_dataset(\"VaibhavMal/AirQualty_imageConv\", split=\"train\")\n",
    "print(dataset)\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def Extract_num(llm_output):\n",
    "    match = re.search(r'\\[\\\\INST\\]\\s*(\\d+)', llm_output)\n",
    "    # match1 = re.search(r'\\[\\INST\\]\\s*(\\d+)', llm_output)\n",
    "\n",
    "    # Extract the number if a match is found\n",
    "    if match:\n",
    "        number = int(match.group(1))\n",
    "        print(f\"Extracted number: {number}\")\n",
    "        return int(number)\n",
    "    else:\n",
    "        print(f\"Not Found\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "count = 0\n",
    "ground_truth = []\n",
    "output_result = []\n",
    "image_type = \"mtf\"\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    \n",
    "    if image_type == \"scalogram\":\n",
    "        en_query = dataset[i][\"query_scalogram\"]\n",
    "        image = dataset[i][\"image_scalogram\"]\n",
    "    elif image_type == \"mtf\":\n",
    "        en_query = dataset[i][\"query_mtf\"]\n",
    "        image = dataset[i][\"image_mtf\"]\n",
    "    elif image_type == \"spectrogram\":\n",
    "        en_query = dataset[i][\"query_spectrogram\"]\n",
    "        # line_to_remove = \"Analyze the provided Spectrogram of Nitrogen Dioxide day time data for four days.\"\n",
    "        # en_query = en_query.replace(line_to_remove, \"\").strip()\n",
    "        # line_to_remove = \"Analyze the provided Spectrogram of Nitrogen Dioxide night time data for four days.\"\n",
    "        # en_query = en_query.replace(line_to_remove, \"\").strip()\n",
    "        image = dataset[i][\"image_spectogram\"]\n",
    "        # image = Image.open(\"zeros_image.png\")\n",
    "        \n",
    "    # image = dataset[i][\"image\"]\n",
    "    # en_query = dataset[i][\"query\"]\n",
    "    ground_truth.append( int(dataset[i][\"answers\"]) )\n",
    "    # print(en_query)\n",
    "\n",
    "    prompt = f\"[INST] <image>\\n{en_query} [\\INST]\"\n",
    "    max_output_token = 4096\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "    output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "    response = processor.decode(output[0], skip_special_tokens=True)\n",
    "    print(response)\n",
    "    output_result.append( Extract_num(response) )\n",
    "    # if output_result[count] == 0:\n",
    "    #     break\n",
    "    # else:\n",
    "    #     count = count + 1\n",
    "\n",
    "    # mae_error = mae_error + abs(ground_truth - output)\n",
    "    # print(response)\n",
    "\n",
    "# if count == len(dataset):\n",
    "#     mae_error = mae_error / len(dataset)\n",
    "# else:\n",
    "#     print(\"error at count: \" + str(count))\n",
    "#     print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ground_truth)\n",
    "print(output_result)\n",
    "print(len(ground_truth) == len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "mae_error = 0\n",
    "rmse_error = 0\n",
    "count = 0\n",
    "\n",
    "for count in range(len(output_result)):\n",
    "    mae_error = mae_error + abs(ground_truth[count] - output_result[count] )\n",
    "    rmse_error = rmse_error + pow( (ground_truth[count] - output_result[count] ) , 2)\n",
    "    count = count + 1\n",
    "\n",
    "mae_error = mae_error / len(dataset)\n",
    "rmse_error = math.sqrt(rmse_error / len(dataset))\n",
    "# if count == len(dataset):\n",
    "#     mae_error = mae_error / len(dataset)\n",
    "# else:\n",
    "#     print(\"error at count: \" + str(count))\n",
    "#     print(response)\n",
    "print(mae_error)\n",
    "print(rmse_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = dataset[3][\"image\"]\n",
    "# en_query = dataset[3][\"query\"]\n",
    "\n",
    "# prompt = f\"[INST] <image>\\n{en_query} [\\INST]\"\n",
    "# max_output_token = 4096\n",
    "# inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "# output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "# response = processor.decode(output[0], skip_special_tokens=True)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = dataset[0][\"image\"]\n",
    "# en_query = \"Analyze the image and predict what kind of time series transformation it could have been?\"\n",
    "# print(en_query)\n",
    "\n",
    "# prompt = f\"[INST] <image>\\n{en_query} [\\INST]\"\n",
    "# max_output_token = 4096\n",
    "# inputs = processor(prompt, image, return_tensors=\"pt\").to(\"cuda:0\")\n",
    "# output = model.generate(**inputs, max_new_tokens=max_output_token)\n",
    "# response = processor.decode(output[0], skip_special_tokens=True)\n",
    "# response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vai_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
