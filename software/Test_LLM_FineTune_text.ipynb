{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# from trl import SFTTrainer\n",
    "# from transformers import TrainingArguments, DataCollatorForSeq2Seq\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from finetune.lib import prepare_peft_model_n_tokenizer, \\\n",
    "#         prepare_dataset, show_memory_stat, max_seq_length, \\\n",
    "#         seed_everything \n",
    "# from unsloth import FastLanguageModel\n",
    "# from unsloth.chat_templates import train_on_responses_only\n",
    "# import pdb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = \"/home/vaibhav/LLMs-TimeSeries/software/Output_dataset_text_test.json\" \n",
    "\n",
    "# class args1:\n",
    "#     def __init__(self, model, dataset_dir, bs, epoch, max_round, max_steps):\n",
    "#         self.model = model\n",
    "#         self.dataset_dir = dataset_dir\n",
    "#         self.bs = bs\n",
    "#         self.epoch = epoch\n",
    "#         self.max_round = max_round\n",
    "#         self.max_steps = max_steps\n",
    "\n",
    "        \n",
    "# # Creating an instance of the Person class\n",
    "# args = args1(\"unsloth/Llama-3.2-1B-Instruct\", dataset_path, 2, 4, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed = 3407\n",
    "# seed_everything(seed)\n",
    "# model, tokenizer = prepare_peft_model_n_tokenizer(\n",
    "#     model_name=args.model\n",
    "# )\n",
    "# # train_dataset, eval_dataset  = prepare_dataset(tokenizer=tokenizer)\n",
    "# train_dataset, eval_dataset  = prepare_dataset(tokenizer=tokenizer, dataset_dir=args.dataset_dir)\n",
    "\n",
    "# # if args.max_steps != -1:\n",
    "# #     args.epoch = 1\n",
    "# print(train_dataset)\n",
    "# print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer_args = TrainingArguments(\n",
    "#         per_device_train_batch_size = args.bs,\n",
    "#         gradient_accumulation_steps = 4,\n",
    "#         warmup_steps = 5,\n",
    "#         num_train_epochs = args.epoch, # Set this for 1 full training run.\n",
    "#         # max_steps = args.max_steps,\n",
    "#         learning_rate = 2e-4,\n",
    "#         fp16 = not is_bfloat16_supported(),\n",
    "#         bf16 = is_bfloat16_supported(),\n",
    "#         logging_steps = 1,\n",
    "#         optim = \"adamw_8bit\",\n",
    "#         weight_decay = 0.01,\n",
    "#         lr_scheduler_type = \"linear\",\n",
    "#         seed = seed,\n",
    "#         output_dir = \"outputs\",\n",
    "#         report_to = \"wandb\", # Use this for WandB etc\n",
    "#         # add evaluation arguments\n",
    "#         fp16_full_eval = False,\n",
    "#         per_device_eval_batch_size = 2,\n",
    "#         eval_accumulation_steps = 4,\n",
    "#         eval_strategy = \"epoch\",\n",
    "#         eval_steps = 1,\n",
    "#     )\n",
    "\n",
    "# trainer = SFTTrainer(\n",
    "#     model = model,\n",
    "#     tokenizer = tokenizer,\n",
    "#     train_dataset = train_dataset,\n",
    "#     eval_dataset = eval_dataset,\n",
    "#     dataset_text_field = \"text\",\n",
    "#     max_seq_length = max_seq_length,\n",
    "#     data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n",
    "#     dataset_num_proc = 2,\n",
    "#     packing = False, # Can make training 5x faster for short sequences.\n",
    "#     args = trainer_args,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"/home/vaibhav/LLMs-TimeSeries/software/Archive/Llama-3.2-1B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mentioned data is from Air Quality Chemical Multisensory Device which uses tungsten oxide as its sensing material and is designed to detect nitrogen dioxide (NO2) gas. It is deployed in a significantly polluted area, at road level, within an Italian city.  Average value on those four nights is [1066.8, 1029.8, 953.9, 1031.0, 948.0, 919.9, 1085.0, 1169.1, 1147.0, 1102.0, 958.0, 946.2], [946.8, 952.1, 939.2, 996.9, 1017.2, 936.2, 846.9, 914.9, 917.2, 943.9, 870.2, 898.1], [834.0, 843.1, 875.8, 831.8, 825.1, 788.0, 819.0, 875.2, 892.1, 804.9, 782.1, 781.8] and [777.1, 760.0, 748.9, 737.9, 726.1, 752.9, 989.1, 1025.0, 1017.1, 906.1, 842.2, 831.0]. Estimate the expected average Nitrogen Dioxide value for the subsequent night.\n",
      "791\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def read_json(json_file):\n",
    "    with open(json_file, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data \n",
    "\n",
    "\n",
    "dataset_dir = \"/home/vaibhav/LLMs-TimeSeries/software/Output_dataset_text_test.json\" \n",
    "\n",
    "dataset = read_json(dataset_dir)\n",
    "print(dataset[0]['user'])\n",
    "print(dataset[0]['assistant'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1002\n",
      "1578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322\n",
      "1579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872\n",
      "1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1832\n",
      "1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1872\n",
      "1539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1762\n",
      "1872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322\n",
      "1459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322\n",
      "1572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1342\n",
      "1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1829\n",
      "1172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1022\n",
      "1022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1532\n",
      "1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1290\n",
      "1322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1322\n",
      "1322\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "ground_truth = []\n",
    "output_result = []\n",
    "image_type = \"spectrogram\"\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "        \n",
    "    # image = dataset[i][\"image\"]\n",
    "    # en_query = dataset[i][\"query\"]\n",
    "    ground_truth.append( int(dataset[i][\"assistant\"]) )\n",
    "    # print(en_query)\n",
    "\n",
    "    prompt = dataset[i]['user']\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")  # Assuming you have a CUDA device\n",
    "    with torch.no_grad():\n",
    "        output_ids = model.generate(input_ids, num_beams=4, max_length=1024)\n",
    "    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    print(int(generated_text[-4:]))\n",
    "    output_result.append( int(generated_text[-4:]) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[791, 1618, 1106, 1768, 1263, 1230, 2011, 1711, 1325, 1683, 1892, 1908, 1664, 1829, 1490, 1436, 1460, 1601, 1423, 1171, 1550, 1212, 1039, 895, 1294, 1778, 1182, 1290, 1261, 1253, 1095]\n",
      "[1002, 1322, 1002, 1578, 1322, 1579, 1872, 1872, 1832, 1839, 1872, 1539, 1762, 1872, 1322, 1459, 1322, 1572, 1342, 1342, 1829, 1172, 1022, 1022, 1532, 1832, 1182, 1290, 1322, 1322, 1322]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(ground_truth)\n",
    "print(output_result)\n",
    "print(len(ground_truth) == len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142.70967741935485\n",
      "186.55155821104188\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "mae_error = 0\n",
    "rmse_error = 0\n",
    "count = 0\n",
    "\n",
    "for count in range(len(output_result)):\n",
    "    mae_error = mae_error + abs(ground_truth[count] - output_result[count] )\n",
    "    rmse_error = rmse_error + pow( (ground_truth[count] - output_result[count] ) , 2)\n",
    "    count = count + 1\n",
    "\n",
    "mae_error = mae_error / len(dataset)\n",
    "rmse_error = math.sqrt(rmse_error / len(dataset))\n",
    "# if count == len(dataset):\n",
    "#     mae_error = mae_error / len(dataset)\n",
    "# else:\n",
    "#     print(\"error at count: \" + str(count))\n",
    "#     print(response)\n",
    "print(mae_error)\n",
    "print(rmse_error)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vai_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
