{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is connected to GPU.\n",
      "GPU Device Name: NVIDIA H100 NVL\n",
      "Number of GPUs available: 4\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is connected to GPU.\")\n",
    "    print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"PyTorch is not connected to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 4096\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "REPO_ID = \"VaibhavMal/llava_v1.6-7b-ASIDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO create Zero images for Llava testing with pure text models\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create a 2D array of zeros\n",
    "width = 400\n",
    "height = 300\n",
    "image_array = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "# Convert the array to an image\n",
    "image = Image.fromarray(image_array)\n",
    "\n",
    "# Save the image\n",
    "image.save(\"zeros_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Dict\n",
    "import random\n",
    "\n",
    "class LlavaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LLaVa. This class takes a HuggingFace Dataset as input.\n",
    "\n",
    "    Each row, consists of image path(png/jpg/jpeg) and ground truth data (json/jsonl/txt).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        split: str = \"train\",\n",
    "        img_type = \"scalogrm\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.answer_token_sequences = []\n",
    "        self.query_list = []\n",
    "        self.image_list = []\n",
    "        for sample in self.dataset:\n",
    "            if \"answers\" in sample:\n",
    "                # assert isinstance(sample[\"answers\"], list)\n",
    "                if img_type == \"scalogram\":\n",
    "                    self.answer_token_sequences.append(sample[\"answers\"])\n",
    "                    self.query_list.append(sample[\"query_scalogram\"])\n",
    "                    self.image_list.append(sample[\"image_scalogram\"])\n",
    "                elif img_type == \"mtf\":\n",
    "                    self.answer_token_sequences.append(sample[\"answers\"])\n",
    "                    self.query_list.append(sample[\"query_mtf\"])\n",
    "                    self.image_list.append(sample[\"image_mtf\"])\n",
    "                elif img_type == \"spectrogram\":\n",
    "                    self.answer_token_sequences.append(sample[\"answers\"])\n",
    "                    line_str = sample[\"query_spectrogram\"]\n",
    "                    # line_to_remove = \"Analyze the provided Spectrogram of Nitrogen Dioxide day time data for four days.\"\n",
    "                    # line_str = line_str.replace(line_to_remove, \"\").strip()\n",
    "                    # line_to_remove = \"Analyze the provided Spectrogram of Nitrogen Dioxide night time data for four days.\"\n",
    "                    # line_str = line_str.replace(line_to_remove, \"\").strip()\n",
    "                    self.query_list.append(line_str)\n",
    "                    # self.image_list.append(Image.open(\"zeros_image.png\"))\n",
    "                    self.image_list.append(sample[\"image_spectogram\"])\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns one item of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            image : the original Receipt image\n",
    "            target_sequence : tokenized ground truth sequence\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "        # inputs\n",
    "        # image = sample[\"image\"]\n",
    "        image = self.image_list[idx]\n",
    "        en_query = self.query_list[idx]\n",
    "        # target_sequence = random.choice(self.answer_token_sequences[idx]) # can be more than one, e.g., DocVQA Task 1\n",
    "        target_sequence = self.answer_token_sequences[idx]\n",
    "        return image, en_query, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F7F2C5EF510>, ' Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday  during the Winter 2016 season is 145, 298, 307, 368, 258, 238, 249, 347, 246, 201. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday.', '287')\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480 at 0x7F7F2C5EF510>\n"
     ]
    }
   ],
   "source": [
    "image_type = \"mtf\"\n",
    "\n",
    "train_dataset = LlavaDataset(\"VaibhavMal/AirQualty_imageConv_2\",  split=\"train\", img_type = image_type)\n",
    "val_dataset = LlavaDataset(\"VaibhavMal/AirQualty_imageConv_2\", split=\"test2\", img_type = image_type)\n",
    "img , en_query, ground_truth = train_dataset[0]\n",
    "print(train_dataset[0])\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday  during the Winter 2016 season is 145, 298, 307, 368, 258, 238, 249, 347, 246, 201. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday. --- Answer: 287\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Tuesday to Thursday  during the Winter 2016 season is 394, 474, 357, 226, 275, 333, 315, 259, 235, 414. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Friday. --- Answer: 355\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Saturday to Monday  during the Winter 2016 season is 381, 271, 218, 329, 320, 497, 430, 317, 79, 227. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Tuesday. --- Answer: 269\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Wednesday to Saturday  during the Winter 2016 season is 179, 198, 153, 43, 4, 292, 588, 742, 587, 507. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Sunday. --- Answer: 411\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Monday to Wednesday  during the Winter 2016 season is 306, 387, 352, 289, 284, 291, 307, 371, 462, 214. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Thursday. --- Answer: 479\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Friday to Sunday  during the Winter 2016 season is 485, 363, 288, 492, 545, 503, 435, 353, 215, 88. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Monday. --- Answer: 208\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Tuesday to Friday  during the Winter 2016 season is 256, 411, 250, 126, 107, 197, 388, 311, 472, 320. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Saturday. --- Answer: 212\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Sunday to Tuesday  during the Winter 2016 season is 245, 214, 230, 222, 289, 406, 219, 116, 145, 193. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Wednesday. --- Answer: 181\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Thursday to Saturday  during the Spring 2016 season is 269, 231, 399, 322, 393, 455, 476, 389, 232, 190. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Sunday. --- Answer: 149\n",
      "Question:  Value on Nitrogen Dioxide in Los Angeles on ten consecutive days from Monday to Friday  during the Spring 2016 season is 209, 221, 193, 192, 210, 164, 116, 503, 426, 224. Analyze the provided Markov Transition Field of Nitrogen Dioxide for ten days and estimate the expected Nitrogen Dioxide value for the subsequent day that is Saturday. --- Answer: 95\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for idx in range(len(train_dataset)):\n",
    "    image, en_query, target_sequence = train_dataset[idx]\n",
    "    print(\"Question:\", en_query, \"--- Answer:\", target_sequence)\n",
    "    counter +=1\n",
    "    if counter == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    images = []\n",
    "    texts = []\n",
    "    for example in examples:\n",
    "        image, en_query, ground_truth = example\n",
    "        images.append(image)\n",
    "        prompt = f\"[INST] <image>\\n{en_query} [\\INST] {ground_truth}\"\n",
    "        texts.append(prompt)\n",
    "\n",
    "    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    image_sizes = batch[\"image_sizes\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, image_sizes, labels\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    # we only feed the prompt to the model\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        image, en_query, ground_truth = example\n",
    "        images.append(image)\n",
    "        prompt = f\"[INST] <image>\\n{en_query} [\\INST] {ground_truth}\"\n",
    "        texts.append(prompt)\n",
    "        answers.append(ground_truth)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    image_sizes = batch[\"image_sizes\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, image_sizes, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LlavaModelPLModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning module for training and validating a multimodal model that processes images and text.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Configuration dictionary containing model hyperparameters and settings.\n",
    "        processor (object): A processor object for handling text and image pre-processing.\n",
    "        model (torch.nn.Module): The model to be trained and evaluated.\n",
    "\n",
    "    Methods:\n",
    "        training_step(batch, batch_idx):\n",
    "            Executes a single training step, computing the loss and logging it.\n",
    "        \n",
    "        validation_step(batch, batch_idx, dataset_idx=0):\n",
    "            Executes a single validation step, generating predictions, comparing them to ground truth, and logging the normalized edit distance.\n",
    "        \n",
    "        configure_optimizers():\n",
    "            Sets up the optimizer and optionally, learning rate scheduler for the training process.\n",
    "        \n",
    "        train_dataloader():\n",
    "            Returns a DataLoader for the training dataset.\n",
    "        \n",
    "        val_dataloader():\n",
    "            Returns a DataLoader for the validation dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single step of training.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing input_ids, attention_mask, pixel_values, image_sizes, and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pixel_values=pixel_values,\n",
    "                            image_sizes=image_sizes,\n",
    "                            labels=labels\n",
    "                          )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        \"\"\"\n",
    "        Performs a single step of validation, generating predictions and computing the normalized edit distance.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing input_ids, attention_mask, pixel_values, image_sizes, and answers.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            dataset_idx (int, optional): Index of the dataset in case of multiple datasets. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of normalized edit distances between predictions and ground truth answers.\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                       pixel_values=pixel_values, image_sizes=image_sizes, max_new_tokens=MAX_LENGTH)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The optimizer for training.\n",
    "        \"\"\"\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the training dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The DataLoader for the training dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The DataLoader for the validation dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(val_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 2,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 1,\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "          \"num_workers\": 4\n",
    "}\n",
    "\n",
    "model_module = LlavaModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Log in to Hugging Face Hub\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model_save_loc = \"/home/vaibhav/LLMs-TimeSeries/software/Model_Dataset_v2_LA_10_mtf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA H100 NVL') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 3.9 B  | train\n",
      "--------------------------------------------\n",
      "22.2 M    Trainable params\n",
      "3.9 B     Non-trainable params\n",
      "3.9 B     Total params\n",
      "15,751.029Total estimated model params size (MB)\n",
      "2962      Modules in train mode\n",
      "756       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 131/131 [01:18<00:00,  1.68it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 203\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 392\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 289\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1 \n",
      "    Answer: 78\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 156\n",
      " Normed ED: 1.0\n",
      "Epoch 1: 100%|██████████| 131/131 [01:17<00:00,  1.69it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: . \n",
      "    Answer: 203\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: . \n",
      "    Answer: 392\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: . \n",
      "    Answer: 289\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1. \n",
      "    Answer: 78\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: . \n",
      "    Answer: 156\n",
      " Normed ED: 1.0\n",
      "Epoch 1: 100%|██████████| 131/131 [01:19<00:00,  1.64it/s, v_num=0]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 131/131 [01:28<00:00,  1.49it/s, v_num=0]\n",
      "CPU times: user 1min 55s, sys: 1min, total: 2min 56s\n",
      "Wall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        default_root_dir=Model_save_loc,  # Set the local director\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0,\n",
    "        # logger=wandb_logger,\n",
    "        callbacks=[\n",
    "            # PushToHubCallback(),\n",
    "            early_stop_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Hugging Face model\n",
    "model.save_pretrained(Model_save_loc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vai_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
