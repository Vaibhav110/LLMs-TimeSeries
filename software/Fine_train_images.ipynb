{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig, LlavaNextForConditionalGeneration, AutoProcessor\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import json\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch is connected to GPU.\n",
      "GPU Device Name: NVIDIA H100 NVL\n",
      "Number of GPUs available: 4\n",
      "Current GPU: 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"PyTorch is connected to GPU.\")\n",
    "    print(f\"GPU Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Number of GPUs available: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.current_device()}\")\n",
    "else:\n",
    "    print(\"PyTorch is not connected to GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 4096\n",
    "MODEL_ID = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "REPO_ID = \"VaibhavMal/llava_v1.6-7b-ASIDataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. \n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\" # during training, one always uses padding on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]\n"
     ]
    }
   ],
   "source": [
    "USE_LORA = False\n",
    "USE_QLORA = True\n",
    "\n",
    "## Load model\n",
    "\n",
    "# Three options for training, from the lowest precision training to the highest precision training:\n",
    "# - QLora\n",
    "# - Standard Lora\n",
    "# - Full fine-tuning\n",
    "if USE_QLORA or USE_LORA:\n",
    "    if USE_QLORA:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        quantization_config=bnb_config,\n",
    "    )\n",
    "else:\n",
    "    # for full fine-tuning, we can speed up the model using Flash Attention\n",
    "    # only available on certain devices, see https://github.com/Dao-AILab/flash-attention?tab=readme-ov-file#installation-and-features\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        torch_dtype=torch.float16,\n",
    "        _attn_implementation=\"flash_attention_2\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = torch.nn.Linear\n",
    "    lora_module_names = set()\n",
    "    multimodal_keywords = ['multi_modal_projector', 'vision_model']\n",
    "    for name, module in model.named_modules():\n",
    "        if any(mm_keyword in name for mm_keyword in multimodal_keywords):\n",
    "            continue\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=find_all_linear_names(model),\n",
    "    init_lora_weights=\"gaussian\",\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import Any, Dict\n",
    "import random\n",
    "\n",
    "class LlavaDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for LLaVa. This class takes a HuggingFace Dataset as input.\n",
    "\n",
    "    Each row, consists of image path(png/jpg/jpeg) and ground truth data (json/jsonl/txt).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset_name_or_path: str,\n",
    "        split: str = \"train\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.split = split\n",
    "\n",
    "        self.dataset = load_dataset(dataset_name_or_path, split=self.split)\n",
    "        self.dataset_length = len(self.dataset)\n",
    "\n",
    "        self.answer_token_sequences = []\n",
    "        self.query_list = []\n",
    "        for sample in self.dataset:\n",
    "            if \"answers\" in sample:\n",
    "                # assert isinstance(sample[\"answers\"], list)\n",
    "                self.answer_token_sequences.append(sample[\"answers\"])\n",
    "                self.query_list.append(sample[\"query\"])\n",
    "\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.dataset_length\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Returns one item of the dataset.\n",
    "\n",
    "        Returns:\n",
    "            image : the original Receipt image\n",
    "            target_sequence : tokenized ground truth sequence\n",
    "        \"\"\"\n",
    "        sample = self.dataset[idx]\n",
    "\n",
    "        # inputs\n",
    "        image = sample[\"image\"]\n",
    "        en_query = self.query_list[idx]\n",
    "        # target_sequence = random.choice(self.answer_token_sequences[idx]) # can be more than one, e.g., DocVQA Task 1\n",
    "        target_sequence = self.answer_token_sequences[idx]\n",
    "        return image, en_query, target_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LlavaDataset(\"VaibhavMal/AirQualty_imageConv\",  split=\"train\")\n",
    "val_dataset = LlavaDataset(\"VaibhavMal/AirQualty_imageConv\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1822, 1887, 1793 and 1650. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 1553\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide four day time data. Average value on those four days is 704, 874, 1172 and 1415. Estimate the expected average Nitrogen Oxide value for the subsequent day. --- Answer: 990\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1488, 1264, 1162 and 1474. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 1471\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1477, 1546, 1795 and 1730. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 1642\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide four day time data. Average value on those four days is 1833, 1786, 1715 and 1424. Estimate the expected average Nitrogen Oxide value for the subsequent day. --- Answer: 1262\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide four day time data. Average value on those four days is 1133, 922, 814 and 903. Estimate the expected average Nitrogen Oxide value for the subsequent day. --- Answer: 956\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1029, 931, 829 and 842. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 791\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1321, 1308, 1183 and 925. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 982\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide four day time data. Average value on those four days is 1868, 1818, 1949 and 1151. Estimate the expected average Nitrogen Oxide value for the subsequent day. --- Answer: 1171\n",
      "Question: Analyze the provided Markov Transition Field of Nitrogen Oxide night time data. Average value on those four nights is 1044, 911, 1100 and 1014. Estimate the expected average Nitrogen Oxide value for the subsequent night. --- Answer: 1023\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for idx in range(len(train_dataset)):\n",
    "    image, en_query, target_sequence = train_dataset[idx]\n",
    "    print(\"Question:\", en_query, \"--- Answer:\", target_sequence)\n",
    "    counter +=1\n",
    "    if counter == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(examples):\n",
    "    images = []\n",
    "    texts = []\n",
    "    for example in examples:\n",
    "        image, en_query, ground_truth = example\n",
    "        images.append(image)\n",
    "        prompt = f\"[INST] <image>\\n{en_query} [\\INST] {ground_truth}\"\n",
    "        texts.append(prompt)\n",
    "\n",
    "    batch = processor(text=texts, images=images, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors=\"pt\")\n",
    "\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = labels\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    image_sizes = batch[\"image_sizes\"]\n",
    "    labels = batch[\"labels\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, image_sizes, labels\n",
    "\n",
    "def eval_collate_fn(examples):\n",
    "    # we only feed the prompt to the model\n",
    "    images = []\n",
    "    texts = []\n",
    "    answers = []\n",
    "    for example in examples:\n",
    "        image, en_query, ground_truth = example\n",
    "        images.append(image)\n",
    "        prompt = f\"[INST] <image>\\n{en_query} [\\INST] {ground_truth}\"\n",
    "        texts.append(prompt)\n",
    "        answers.append(ground_truth)\n",
    "\n",
    "    batch = processor(text=texts, images=images, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "    pixel_values = batch[\"pixel_values\"]\n",
    "    image_sizes = batch[\"image_sizes\"]\n",
    "\n",
    "    return input_ids, attention_mask, pixel_values, image_sizes, answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from torch.utils.data import DataLoader\n",
    "import re\n",
    "from nltk import edit_distance\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LlavaModelPLModule(L.LightningModule):\n",
    "    \"\"\"\n",
    "    A PyTorch Lightning module for training and validating a multimodal model that processes images and text.\n",
    "\n",
    "    Attributes:\n",
    "        config (dict): Configuration dictionary containing model hyperparameters and settings.\n",
    "        processor (object): A processor object for handling text and image pre-processing.\n",
    "        model (torch.nn.Module): The model to be trained and evaluated.\n",
    "\n",
    "    Methods:\n",
    "        training_step(batch, batch_idx):\n",
    "            Executes a single training step, computing the loss and logging it.\n",
    "        \n",
    "        validation_step(batch, batch_idx, dataset_idx=0):\n",
    "            Executes a single validation step, generating predictions, comparing them to ground truth, and logging the normalized edit distance.\n",
    "        \n",
    "        configure_optimizers():\n",
    "            Sets up the optimizer and optionally, learning rate scheduler for the training process.\n",
    "        \n",
    "        train_dataloader():\n",
    "            Returns a DataLoader for the training dataset.\n",
    "        \n",
    "        val_dataloader():\n",
    "            Returns a DataLoader for the validation dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, processor, model):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.processor = processor\n",
    "        self.model = model\n",
    "\n",
    "        self.batch_size = config.get(\"batch_size\")\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Performs a single step of training.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing input_ids, attention_mask, pixel_values, image_sizes, and labels.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss for the batch.\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, labels = batch\n",
    "\n",
    "        outputs = self.model(input_ids=input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            pixel_values=pixel_values,\n",
    "                            image_sizes=image_sizes,\n",
    "                            labels=labels\n",
    "                          )\n",
    "        loss = outputs.loss\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, dataset_idx=0):\n",
    "        \"\"\"\n",
    "        Performs a single step of validation, generating predictions and computing the normalized edit distance.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): A tuple containing input_ids, attention_mask, pixel_values, image_sizes, and answers.\n",
    "            batch_idx (int): The index of the current batch.\n",
    "            dataset_idx (int, optional): Index of the dataset in case of multiple datasets. Defaults to 0.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of normalized edit distances between predictions and ground truth answers.\n",
    "        \"\"\"\n",
    "\n",
    "        input_ids, attention_mask, pixel_values, image_sizes, answers = batch\n",
    "\n",
    "        # autoregressively generate token IDs\n",
    "        generated_ids = self.model.generate(input_ids=input_ids, attention_mask=attention_mask,\n",
    "                                       pixel_values=pixel_values, image_sizes=image_sizes, max_new_tokens=MAX_LENGTH)\n",
    "        # turn them back into text, chopping of the prompt\n",
    "        # important: we don't skip special tokens here, because we want to see them in the output\n",
    "        predictions = self.processor.batch_decode(generated_ids[:, input_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "        scores = []\n",
    "        for pred, answer in zip(predictions, answers):\n",
    "            pred = re.sub(r\"(?:(?<=>) | (?=</s_))\", \"\", pred)\n",
    "            scores.append(edit_distance(pred, answer) / max(len(pred), len(answer)))\n",
    "\n",
    "            if self.config.get(\"verbose\", False) and len(scores) == 1:\n",
    "                print(f\"Prediction: {pred}\")\n",
    "                print(f\"    Answer: {answer}\")\n",
    "                print(f\" Normed ED: {scores[0]}\")\n",
    "\n",
    "        self.log(\"val_edit_distance\", np.mean(scores))\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Configures the optimizer for training.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim.Optimizer: The optimizer for training.\n",
    "        \"\"\"\n",
    "        # you could also add a learning rate scheduler if you want\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.config.get(\"lr\"))\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the training dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The DataLoader for the training dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        \"\"\"\n",
    "        Returns the DataLoader for the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "            DataLoader: The DataLoader for the validation dataset.\n",
    "        \"\"\"\n",
    "        return DataLoader(val_dataset, collate_fn=eval_collate_fn, batch_size=self.batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"max_epochs\": 2,\n",
    "          # \"val_check_interval\": 0.2, # how many times we want to validate during an epoch\n",
    "          \"check_val_every_n_epoch\": 1,\n",
    "          \"gradient_clip_val\": 1.0,\n",
    "          \"accumulate_grad_batches\": 8,\n",
    "          \"lr\": 1e-4,\n",
    "          \"batch_size\": 1,\n",
    "          # \"seed\":2022,\n",
    "          \"num_nodes\": 1,\n",
    "          \"warmup_steps\": 50,\n",
    "          \"result_path\": \"./result\",\n",
    "          \"verbose\": True,\n",
    "          \"num_workers\": 4\n",
    "}\n",
    "\n",
    "model_module = LlavaModelPLModule(config, processor, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import Callback\n",
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "\n",
    "class PushToHubCallback(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub, epoch {trainer.current_epoch}\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training in progress, epoch {trainer.current_epoch}\")\n",
    "    def on_train_end(self, trainer, pl_module):\n",
    "        print(f\"Pushing model to the hub after training\")\n",
    "        pl_module.processor.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "        pl_module.model.push_to_hub(REPO_ID,\n",
    "                                    commit_message=f\"Training done\")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_edit_distance\", patience=3, verbose=False, mode=\"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "# Log in to Hugging Face Hub\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA H100 NVL') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | PeftModel | 3.9 B  | train\n",
      "--------------------------------------------\n",
      "22.2 M    Trainable params\n",
      "3.9 B     Non-trainable params\n",
      "3.9 B     Total params\n",
      "15,751.029Total estimated model params size (MB)\n",
      "2962      Modules in train mode\n",
      "756       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 130/130 [01:22<00:00,  1.58it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Expanding inputs for image tokens in LLaVa-NeXT should be done in processing. Please add `patch_size` and `vision_feature_select_strategy` to the model's processing config or set directly with `processor.patch_size = {{patch_size}}` and processor.vision_feature_select_strategy = {{vision_feature_select_strategy}}`. Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: .4 \n",
      "    Answer: 1294\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaibhav/miniconda3/envs/vai_llama/lib/python3.11/site-packages/lightning/pytorch/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: .4 \n",
      "    Answer: 1892\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 1117\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: .5 \n",
      "    Answer: 895\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: .6 \n",
      "    Answer: 1357\n",
      " Normed ED: 1.0\n",
      "Epoch 1: 100%|██████████| 130/130 [01:22<00:00,  1.58it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \\] \n",
      "    Answer: 1294\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \\* \n",
      "    Answer: 1892\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 1117\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \n",
      "    Answer: 895\n",
      " Normed ED: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: \\* \n",
      "    Answer: 1357\n",
      " Normed ED: 1.0\n",
      "Epoch 1: 100%|██████████| 130/130 [01:24<00:00,  1.54it/s, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 130/130 [01:32<00:00,  1.40it/s, v_num=1]\n",
      "CPU times: user 2min 2s, sys: 1min 2s, total: 3min 4s\n",
      "Wall time: 3min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "trainer = L.Trainer(\n",
    "        default_root_dir=\"/home/vaibhav/LLMs-TimeSeries/software/Checkpoint_img_model_v3/\",  # Set the local director\n",
    "        accelerator=\"gpu\",\n",
    "        devices=[0],\n",
    "        max_epochs=config.get(\"max_epochs\"),\n",
    "        accumulate_grad_batches=config.get(\"accumulate_grad_batches\"),\n",
    "        check_val_every_n_epoch=config.get(\"check_val_every_n_epoch\"),\n",
    "        gradient_clip_val=config.get(\"gradient_clip_val\"),\n",
    "        precision=\"16-mixed\",\n",
    "        limit_val_batches=5,\n",
    "        num_sanity_val_steps=0,\n",
    "        # logger=wandb_logger,\n",
    "        callbacks=[\n",
    "            # PushToHubCallback(),\n",
    "            early_stop_callback],\n",
    ")\n",
    "\n",
    "trainer.fit(model_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Hugging Face model\n",
    "model.save_pretrained(\"/home/vaibhav/LLMs-TimeSeries/software/Checkpoint_img_model_v3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vai_llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
